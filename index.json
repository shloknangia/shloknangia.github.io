[{"content":"Streamlining Data Manipulation with the power of Command Line tools Authors: Linsong, Shlok Nangia, Jialiang Guo (3_datamen)\nThis blog is written and maintained by students in the Master of Science in Professional Computer Science Program at Simon Fraser University as part of their course credit. To learn more about this unique program, please visit sfu.ca/computing/mpcs.\nDo you know you can ‚Äúclean and manipulate data‚Äù from a document without even opening the documents. Yes, you read that right! No need for any text editor.\nNot only that, most of the data cleaning and manipulations tasks can be performed by command line tools by writing just 1 line. Just think of the time you can save. No need to open the document in some text editor or excel and rely on the limited functionalities they give you.\nPhoto by Gabriel Heinzer on Unsplash\nSo, if you are also a geek like us who loves to work on terminal or would like to learn how to do all data cleaning, extraction, and aggregation operations in terminal(so that you can look like a hacker üòâ), this article is for you.\nHere we are going to look at some powerful command line tools such as ‚Äòsed‚Äô and ‚Äòawk‚Äô which can simplify and speed up your data cleaning process. After reading this article you will be able to harness the power of these versatile tools to do efficient and effective data processing.\nIntroduction sed (Stream Editor): A streamline editor which performs operations on a stream of text without opening it in any text editor.\nawk(created by Aho, Weinberger \u0026amp; Kernighan): A data-driven programming language to process and analyze text.\ngrep(Global Regular Expression Print): A command line tool to quickly search(patterns and regular expressions) and filter text data from one or multiple files.\nCombination of above tools : Although each of the above tools are powerful enough but a combination of these can make many complex operations easy. For example:- When using extremely large datasets, firstly, sed can be used to search and replace some text and then, awk to extract some meaningful data and perform complex calculations, which will automate the time consuming complex tasks and make you more efficient.\nInstallation For Mac and Linux Users: If you are using ‚ÄúMac OS‚Äù or a ‚ÄúLinux‚Äù distribution, you already have these amazing tools installed in your system. Open up the ‚Äòterminal‚Äô and try the following commands to invoke these tools.\n$ sed \u0026ndash;help $ grep \u0026ndash;help $ awk \u0026ndash;help\nFor windows Users: You can install these tools from given links, but we recommend to use ‚ÄòGit Bash‚Äô or ‚ÄòWSL(Windows Subsystem for Linux)‚Äô , where you can find these tools preinstalled.\nsed for Windows grep for Windows Gawk for Windows Motivation Following are few more use cases where the combination of above tools can be used.\nData cleaning: Removing unwanted characters, formatting data, and fixing errors.\nData extraction: Selecting specific data based on certain conditions.\nData transformation: Converting data from one format to another.\nData aggregation: Calculating statistics and aggregating data.\nLet‚Äôs Dig Deeper 1. grep : global regular expression print This tool is the goto tool to perform quick search operations in the files while working on terminal or command line. It can also be used to search and filter data in multiple files with the use of regular expressions. Let‚Äôs look at some examples.\nExamples:\nFind all occurrences of the pattern ‚Äòpatricia‚Äô in a file:\n$ grep 'patricia' myfile Same as above but looking only for complete words:\n$ grep -w 'patricia' myfile Find all occurrences of the pattern ‚Äò.Pp‚Äô at the beginning of a line:\n$ grep '^\\.Pp' myfile Find all lines in a file which do not contain the words ‚Äòfoo‚Äô or ‚Äòbar‚Äô:\n$ grep -v -e 'foo' -e 'bar' myfile 2. AWK As mentioned before, ‚Äòawk‚Äô is a data driven programming language which is mainly used for text processing, data extraction and reporting tool. It was created by Aho, Weinberger \u0026amp; Kernighan(hence the name AWK). This powerful tool allows us to parse the data from the files and perform various operations on them. It also allows to search for patterns and perform minor calculations on the data.\nExamples:\nPrint lines longer than 72 characters.\n$ length($0) \u0026gt; 72 Print first two fields in opposite order.\n{ print $2, $1 } Same, with input fields separated by comma and/or spaces and tabs.\nBEGIN { FS = \u0026quot;,[ \\t]*|[ \\t]+\u0026quot; } { print $2, $1 } { s += $1 } END { print \u0026quot;sum is\u0026quot;, s, \u0026quot; average is\u0026quot;, s/NR } Add up first column, print sum and average. /start/, /stop/ Print all lines between start/stop pairs.\nBEGIN { # Simulate echo(1) for (i = 1; i \u0026lt; ARGC; i++) printf \u0026quot;%s \u0026quot;, ARGV[i] printf \u0026quot;\\n\u0026quot; exit } 3. sed : stream editor As we mentioned before ‚Äòsed‚Äô is a stream editor i.e. a command line tool which is used for editing file. It can be used for multiple operations such as insert or delete lines , search or replace for specific words or text(regex) patterns. The power of this tool allows us to even edit multiple text files at the same time. Let‚Äôs see some examples to truly understand it.\nExamples:\nReplace ‚Äòbar‚Äô with ‚Äòbaz‚Äô when piped from another command:\n$ echo \u0026quot;An alternate word, like bar, is sometimes used in examples.\u0026quot; | sed 's/bar/baz/' Using backlashes can sometimes be hard to read and follow:\n$ echo \u0026quot;/home/example\u0026quot; | sed 's/\\/home\\/example/\\/usr\\/local\\/example/' Using a different separator can be handy when working with paths:\n$ echo \u0026quot;/home/example\u0026quot; | sed 's#/home/example#/usr/local/example#' Replace all occurances of ‚Äòfoo‚Äô with ‚Äòbar‚Äô in the file test.txt, without creating a backup of the file:\n$ sed -i '' -e 's/foo/bar/g' test.txt 4. The ‚Äú|‚Äù (pipe) symbol The last tool which we will be covering today is ‚Äú|‚Äù i.e. pipe symbol. A lot of people don‚Äôt know any use of this tool, but in linux, this symbol is used to link 2 commands or more specifically to direct the output of one command and serve it as a input to another command.\nFor example: using ls | grepgets the list of contents in the directory(i.e. output of ‚Äòls‚Äô) and sends it as a input to ‚Äògrep‚Äô command.\nNow let us get our hands dirty Imagine you are a data scientist working at a real-estate company. You download a property_tax_report from this webpage. The dataset contains information on properties from BC Assessment (BCA) and City sources in 2021.\nYou may think that for a newly built house, it tends to have a higher price than the ones built decades ago. So let us clean the data for this idea.\n1. First glance of the data $ head -2 property-tax-report-2.csv Here head -2 means to print first 2 lines from the data file. With this we could see how the header and the data looks like.\nSince the housing price varies a lot by locations, we will only consider the houses whose postcode starts with ‚ÄòV6A‚Äô. Furthermore, we remove the houses that were built before 1900.\n2. Locate the columns you what to use the filter $ head -2 property-tax-report-2.csv|awk -F';' '{print$(NF-4),$14}' Here the print(NF‚àí4),14 means we only print the YEAR_BUILT and the PROPERTY_POSTAL_CODE columns.\n3. Check the top 10 of the buildings number by year $ cat property-tax-report-2.csv|awk -F';' '{print$(NF-4)}'|sort|uniq -c|sort -nr|head -10 Here sort|uniq -c|sort -nr part means we sort the build year and then uniq count the year, so we could have the number of the buildings that build from each year. Then we sort the numbers again to show the top 10.\n4. Make the filter $ head -10000 property-tax-report-2.csv|awk -F';' '{if (($14 ~ /^V6A/)\u0026amp;\u0026amp;($(NF-4)\u0026gt;1900))print$14,$(NF-4)}'|head -5 Here $14 ~ /^V6A/ means column 14 start with V6A. You could also replace above with this:\n$ head -10000 property-tax-report-2.csv|grep 'V6A'|awk -F';' '{print$14,$(NF-4)}'|head -5 seems it works!\n5. Output the result to a file $ cat property-tax-report-2.csv|awk -F';' '{if (($14 ~ /^V6A/)\u0026amp;\u0026amp;($(NF-4)\u0026gt;1900))print$0}'\u0026gt;property-tax-report-filter.csv Just remember the header will be removed after this. To make it become a ‚Äúreal‚Äù comma-separated values file just add: sed ‚Äús/;/,/g‚Äù\n$ cat property-tax-report-2.csv|awk -F';' '{if (($14 ~ /^V6A/)\u0026amp;\u0026amp;($(NF-4)\u0026gt;1900))print$0}'|sed \u0026quot;s/;/,/g\u0026quot;\u0026gt;property-tax-report-filter.csv 6. Last move We create a new column and value it as (CURRENT_LAND_VALUE+ CURRENT_IMPROVEMENT_VALUE)/1000000 then we could study the whether YEAR_BUILT and HOUSE_PRICE are correlated.\n$ head -2 property-tax-report-filter.csv|awk -F';' '{print$(NF-4),($20+$21)/1000000}' Advantages Nowadays, there are so many tools that can help us with data preparation, such as Python, Perl, R, and some even with Graphic User Interface like Microsoft Excel, Jupyter Notebook, Tableau Prep, etc. You may have already mastered some of those modern technologies, so that you might ask:\n‚ÄúWhy should I learn some old-fashioned command line languages, given that learning something new can be such a painful process?‚Äù\nWell, it‚Äôs true that sed and awk are old ‚Äî they‚Äôre already in their 40s ‚Äî but that doesn‚Äôt mean they‚Äôre outdated. In some ways they still perform better than other tools:\n1. They are preinstalled in Unix and Unix-Like Systems Have you ever been in a situation where your lab has just bought a new workstation, or your company‚Äôs IT support team has just updated the workstation and reinstalled system, or you are simply using someone else‚Äôs device, and you need to process a batch of data before the environment and software being configured? It can be a dilemma when your task is urgent or you don‚Äôt want to waste time configuring or waiting for the IT Technician to configure the environment. But don‚Äôt worry, all the command line tools mentioned above are typically preinstalled on Unix-based systems, and you don‚Äôt need to install any extra packages like what you do for Python.\n2. They Are One-Liner-Friendly The syntax of sed and awk is designed to be concise, allowing people to accomplish a lot of actions with relatively few lines of code, or even possibly only one line of code if the programmer is familiar with their features. For the same task, sed/awk code is usually shorter than high-level programming language code. Here is an example of using awk to extract the first column of the input csv file and write it into the output file:\n$ awk -F, '{print $1}' input \u0026gt; output And its equivalent Python implementation:\nwith open(\u0026quot;input_path\u0026quot;, \u0026quot;r\u0026quot;) as f_input, open(\u0026quot;output_path\u0026quot;, \u0026quot;w\u0026quot;) as f_output: for line in f_input: f_output.write(line.split(\u0026quot;,\u0026quot;)[0] + \u0026quot;\\n\u0026quot;) The awk code here costs less effort to write and is more human-readible than the Python implementation. This is a simplest case and for more complex tasks the difference can be even larger.\n3. They Run Fast Sed and awk are not as extensible as modern languages like Python and Perl, and thus can be used for a narrower range of purposes than the latter; however, it is because of such a concentration that they can avoid any features irrelevant to text processing, keep lightweight, and get an well-optimized interpreter, so that for tasks within their capability they are normally faster than high-level programming languages, not to mention other GUI software.\nAnother reason why they‚Äôre fast is that sed uses a simple model of regular expression and awk compiles regular expression into state machine. The former makes it extremely fast for sed to process simple text processing tasks, while the latter helps awk with quickly determining if a line of text matches a complex pattern even for large amounts of text data.\n4. They Are Mouse-Free This one depends on personal habits. Some programmers like GUI and mouses because they make our life easier, while others prefer using keyboard only because mouse sometimes interrupt their flow of thoughts and short cuts is usually faster than mouse clicking.\nIf you are a keyboard lover, then sed and awk is perfect for you: it‚Äôs purely text- and keyboard-based with no need to move or click a mouse.\nFinal Note The real power of command line tools is still unexplored, we have just tried to scratch the surface. There are so many more tools out there which make our day to day work easy such as cut, sort, uniq, wc, tr, etc. But that might be the topic of our future posts.\nTill then , Keep Learning üòä\nReferences Bash Guide for Beginners\nAwk: The Power and Promise of a 40-Year-Old Language\nMan pages: sed\nPipeline_(Unix)\nCity of Vancouver ‚Äî Open Data Portal\nWhat are the differences between Perl, Python, AWK and sed\nWhen To Use Sed Awk Over Perl Or Python\nRegular Expression Matching Can Be Simple And Fast, Russ Cox\n","permalink":"http://localhost:1313/blog/command-line-tools/","summary":"\u003ch1 id=\"streamlining-data-manipulation-with-the-power-of-command-line-tools\"\u003eStreamlining Data Manipulation with the power of Command Line tools\u003c/h1\u003e\n\u003cp\u003eAuthors: Linsong, Shlok Nangia, Jialiang Guo (3_datamen)\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eThis blog is written and maintained by students in the Master of Science in Professional Computer Science Program at Simon Fraser University as part of their course credit. To learn more about this unique program, please visit \u003ca href=\"https://www.sfu.ca/computing/current-students/graduate-students/academic-programs/professional-master-of-science-in-computer-science.html\"\u003esfu.ca/computing/mpcs\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eDo you know you can ‚Äúclean and manipulate data‚Äù from a document without even opening the documents. Yes, you read that right! No need for any text editor.\u003c/p\u003e","title":"Streamlining Data Manipulation with the power of Command Line tools"},{"content":"The Truth Behind Fake News: Tools and Techniques for Detection Authors: Linsong, Shlok Nangia, Jialiang Guo (3_datamen)\nThis blog is written and maintained by students in the Master of Science in Professional Computer Science Program at Simon Fraser University as part of their course credit. To learn more about this unique program, please visit sfu.ca/computing/mpcs.\nMotivation and Background: Recent years have witnessed fake news becoming a major problem, particularly on social media platforms like Twitter and Facebook. Our project aims to identify and classify these fake news and articles, to improve media literacy, protect public from the effects of fake news, and support easier fact-checking.\nProblem Statement: The challenge of fake news detection lies in distinguishing between genuine and false information, particularly as fake news becomes increasingly sophisticated and difficult to detect. Our project aims to answer the following questions:\nCan we accurately identify fake news using machine learning techniques?\nHow can we improve the accuracy and reliability of fake news detection?\nWhat features or characteristics of news articles are most indicative of fake news?\nOur Data Science Pipeline: Our data science pipeline involves several components, including data collection, preprocessing, feature extraction, model training, and evaluation. We collected a dataset of news articles from various sources, both genuine and fake, and preprocessed the data by removing irrelevant information and cleaning the text.\nData Collection: The initial stage of our data science pipeline involves gathering raw data from various sources. In this project, we have selected the PHEME, Liar, and Buzzfeed News datasets as the primary sources for fake news detection. These datasets provide diverse and rich information, making them suitable for our analysis. Additionally, we collect live data from Twitter using the Tweepy library as an up-to-date and dynamic source of information to enhance our model‚Äôs performance and adaptability.\nDownloading the datasets:\nPHEME: Access to the PHEME dataset can be requested through the PHEME project page. Once access is granted, download the dataset files.\nLiar: The Liar dataset can be downloaded from the University of Texas website. Download and unzip the dataset files.\nBuzzfeed News: The Buzzfeed News dataset is available on the BuzzFeed News GitHub repository.\n2. Data Preprocessing:\nIn this stage, the raw data is cleaned and preprocessed to make it suitable for analysis. This process involves several steps, including handling missing values, removing duplicates, correcting inconsistencies, and converting data types. Data preprocessing also includes feature engineering, which entails creating new features from existing ones to better represent the problem domain. We have performed the following preprocessing steps:\nLoad the BuzzFeed, Liar, and PHEME datasets from their respective files.\nFor each dataset, perform the following preprocessing steps:\nRemove any unnecessary columns and keep only the relevant ones (e.g., label and text). Convert the text to lowercase. Remove any non-alphabetic characters. For the BuzzFeed dataset, convert the ‚Äútype‚Äù column to a binary label.\nFor the Liar dataset, convert the ‚Äúlabel‚Äù column to a binary label.\nCombine the preprocessed datasets into a single DataFrame.\nSave the combined preprocessed data to a CSV file for further analysis and modeling.\nWe perform these preprocessing steps, ensuring that the resulting dataset is clean, consistent, and ready for analysis. By applying these data preprocessing techniques, you can ensure that the dataset is suitable for use in machine learning algorithms and other data analysis methods.\n3. Exploratory Data Analysis (EDA):\nEDA is the process of visually and analytically exploring the data to understand its structure, relationships, and patterns. This helps in generating hypotheses, identifying outliers, and determining the most relevant features for the analysis. In this case, we will analyze two datasets:\nthe combined dataset containing rumor and non-rumor threads for various events, and\nThe BuzzFeed dataset containing real and fake news. Common EDA techniques include descriptive statistics, histograms, box plots, and scatter plots.\nWe will perform the following tasks using the provided code as a reference for both datasets:\nCompare the number of different types of threads (rumor and non-rumor) and news (real and fake) for each event and source, respectively. Analyze the dynamics of information spread over time, such as retweets of source tweets in rumor and non-rumor threads.\nExamine the depth of reaction structures for both rumor and non-rumor threads and the relationship between news type and the presence of movies (video links) and images in the news articles.\nVisualize the network structures of rumor and non-rumor threads using graph representations and identify common sources that publish both real and fake news.\nHere, we‚Äôll summarize the insights obtained from the Exploratory Data Analysis:\nThe comparison of rumor and non-rumor threads for each event and the comparison of real and fake news among various sources help us understand the distribution of rumors, non-rumors, real news, and fake news in different scenarios.\nAnalyzing the dynamics of information spread, such as the number of retweets of source tweets in rumor and non-rumor threads over time, helps us understand the impact of rumors on social media and the spread of real and fake news.\nExamining the depth of reaction structures for both rumor and non-rumor threads and the relationship between news type and the presence of movies (video links) and images in the news articles provide insights into the complexity of discussions and the prevalence of visual content in different types of threads and news articles. This can help us identify the level of engagement and how different types of threads and news articles evolve over time.\nVisualizing the network structures of rumor and non-rumor threads using graph representations and identifying common sources that publish both real and fake news allows us to explore the connectivity and interaction patterns among users participating in these threads and assess the credibility and reliability of various sources. This can help us identify influential nodes or clusters in the network that may contribute to the spread of information, rumors, or fake news.\nOverall, the Exploratory Data Analysis helps us to understand the structure and the relationships among the features in both datasets. This understanding can guide us in developing more accurate predictive models for detecting rumors and non-rumors in social media threads and real and fake news in the BuzzFeed dataset.\nTop 5 sources that publish Real news: Buzzfeed\nTop 5 sources that publish Fake news: Buzzfeed\nCommon source of real and fake news in Buzzfeed\nIn addition to analyzing the BuzzFeed dataset, we also explored the connections between speakers in the ‚Äúliar‚Äù dataset based on their shared contexts. We created a directed graph using NetworkX, where nodes represent speakers and edges represent pairs of speakers who appear in the same context. This allowed us to compute various network statistics, such as the number of nodes, the number of edges, and the average degree. Moreover, we visualized the graph using two different layouts: the spring layout (force-directed) and the Circos plot. This analysis helped us understand the relationships between speakers and provided insights into the structure of their interactions. By examining the connections between speakers, we can potentially identify patterns or clusters that may contribute to the spread of misinformation.\n4. Model Monitoring and Maintenance: In this part, we integrated live Twitter data collection and used OpenAI to simulate labels generated by human annotators. To enhance the user experience, we can also incorporate user feedback based on the predicted scores from our model. Here‚Äôs a high-level overview of how to include user feedback in the pipeline:\nAfter deploying the model, users can interact with it by submitting text or links that they want to evaluate for credibility.\nThe model provides a credibility score or prediction for the submitted content.\nUsers can then provide feedback on the prediction, indicating whether they agree or disagree with the model‚Äôs evaluation.\nCollect user feedback and store it in a database, along with the original content and the model‚Äôs prediction.\nPeriodically update the training data with the new feedback and retrain the model to improve its performance. This step can be automated using techniques such as active learning or by manually incorporating the user feedback into the training data.\nBy incorporating user feedback into the model, we ensure that the model continuously adapts to the changing patterns of fake news and misinformation. This approach can lead to better overall performance and improved accuracy in detecting fake news.\nMethodology Data Splitting:\nTo prepare the data for model training and evaluation, we first integrated four datasets from different sources into a single dataset. The resultant dataset consists of 64296 pieces of text with their corresponding labels (True or Fake).\nTo ensure that the dataset was split in a way that could allow us to evaluate the models properly, we used the train_test_split function from the scikit-learn library to divide it into training set, validation set, and test set, in a ratio of 70:15:15.\nWe also set the random_state parameter to ensure that the split is reproducible, so that we can compare different models with each other later, and used the stratify parameter to ensure that each set contains the same proportion of samples of each class.\nModel Selection and Training:\nWe did transfer learning based on three popular natural language processing models on our dataset: BERT, GPT-2, and LSTM, and explored their performance. BERT and GPT-2 are transformer-based models, while LSTM is a recurrent neural network (RNN) model.\nBERT and GPT-2: To utilize the power of these pre-trained transformer models, we first used HuggingFace‚Äôs transformers library to tokenize our training data and encapsulate the tokenized data with corresponding labels into Dataloaders, for batch training.\nThen we load the pre-trained BERT/GPT-2 model, froze their parameters, and added a trainable binary classification head, which consists of two dense layers, a ReLu activation and a Dropout (to prevent overfitting) between them, and a LogSoftmax function at the end. We took the CLS token from the output of the last hidden layer of each transformer and used them as input to the classification head.\nFor the training stage, we adopted PyTorch Lightning Framework to conduct batch training for both models. During training, we logged the training and validation performance metrics including accuracy and loss, which can be visualized using TensorBoard.\n2. LSTM\nFor the LSTM model, we first defined a text transform pipeline to preprocess the text data before training. The pipeline consists of a SentencePieceTokenizer, a VocabTransform, truncation, and adding any extra tokens. Similar to previous section, the transformed data was encapsulated with corresponding labels into Dataloaders.\nAs LSTM is a type of neural network architecture other than transformer-based, it needs word embedding to get continuous, dense vector representations of words with it can work on. We applied pre-trained GloVe-6B word embeddings for this purpose. The neural network architecture consists of the word embedding, the LSTM model which takes the output of embedding as its input, and a dense layer to translate the output of LSTM model into logarithmic probability of two classes.\nAgain, We used PyTorch Lightning Framework to conduct batch training, and logged the training and validation performance of the LSTM model which can be visualized in TensorBoard.\nModel Explanation:\nSo, we built our fake news detector data product using BERT, GPT-2, and LSTM models. Our goal was to demonstrate the effectiveness of these models by employing a Global Surrogate approach: training a logistic regression model using the predicted labels from our models on the training dataset. We further enhanced the interpretability of these black-box models using LIME and SHAP techniques.\nWe used the dataset with pred_labels generated by BERT, GPT-2, and LSTM models to train and evaluate a logistic regression model for fake news detection. We then applied LIME and SHAP techniques to interpret the model. Here‚Äôs a brief explanation of each step:\nLogistic Regression Model:\nLoad the dataset and preprocess it to create binary labels.\nSplit the dataset into train and test sets, and vectorize the text using TfidfVectorizer.\nTrain a logistic regression model and evaluate its performance using the classification report.\nUse the logistic regression model as a surrogate model for the black-box models (BERT, GPT-2, and LSTM) to improve interpretability.\nInterpretability Techniques (LIME and SHAP):\nLIME:\nDefine a function to predict probabilities for the LIME explainer.\nCreate a LimeTextExplainer object and choose an instance from the test set to explain.\nUse the LIME explainer to generate an explanation for the chosen instance.\nVisualize the LIME explanation to understand the impact of each feature (word) on the prediction.\nSHAP:\nCreate a SHAP explainer using the logistic regression model and calculate SHAP values for the test set.\nVisualize the SHAP values using a beeswarm plot to understand the contribution of each feature (word) to the predictions.\nEvaluation Model Validation and Evaluation: The validation is conducted on the validation set during training, as stated above. We conducted experiment among different hyperparameters such as learning rate, dropout rate, layer dimension, optimizer, word embedding/transformer etc. By observing the validation performance, we selected the best performing models and their corresponding hyperparameter, and we were able to decide whether to stop early to prevent overfitting.\nOnce the model has been trained and validated, we can evaluate its performance by using it to make predictions on the test set, which contains unseen samples. We used scikit-learn‚Äôs classification_report method to generate metrics describing accuracy, precision, recall, and F1-score of each model.\nThe GPT-2-based model can generally achieve the best performance over all three models. LSTM-based model can also achieve relatively high accuracy, but its performance is not stable. In some training rounds it can only achieve a 55‚Äì60% accuracy, which may be due to weights initialization. BERT-based model is more stable than LSTM, but can only achieve a accuracy of ~85%. The confusion_matrix and ConfusionMatrixDisplay methods can also be used to visualize the performance of the model and identify areas where it may be making errors.\nFrom Left To Right: Confusion Matrix of BERT-Based, GPT-2 Based, LSTM-Based Model\nInterpretability Results: LIME and SHAP results provide insights into the most important features contributing to the predictions. LIME produces instance-by-instance explanations, which may seem random and harder to interpret. In contrast, SHAP offers a comprehensive evaluation, revealing features like ‚Äúvideo‚Äù positively affect predictions, possibly suggesting that visual evidence makes news more reliable. Features like ‚Äúhttp‚Äù and ‚Äúsays‚Äù negatively affect predictions, indicating online rumors and hearsay might be prevalent in these cases. Analyzing these explanations helps us better understand the factors considered by the models when detecting fake news.\nData Product Our data product is a tool that allows users to enter a news article details such as title, source URL, description and receive a prediction of whether it is genuine or fake. This tool can be integrated/added to different ‚ÄúNews Sharing‚Äù platforms to validate the integrity of a specific article before the article is posted/shared on a platform.\nThe application uses our best-performing machine learning model(s) to make predictions based on the text content of the article and the output probabilities will then be adjusted according to the credibility of source of news. We demonstrated the functionality of the application by testing it on several news articles and comparing the predictions to known ground truth labels.\nThis Screenshot was taken in April 2023\nAnd using the ‚Äúthumbs-up‚Äù and ‚Äúthumbs-down‚Äù buttons, we have also provided options to give feedback to the models if its prediction is correct or not. Which then sends update to our model and this information will be used to re-train our model or extend its training on this live data.\nLessons Learnt Through this project, we learned the importance of carefully selecting and preprocessing data, as well as the benefits and limitations of various machine learning techniques for text classification tasks. We also gained experience in developing web applications and presenting our findings in a clear and concise manner.\nFuture Work As part of our future work, we plan to implement the following enhancements to improve the accuracy and reliability of our fake news detection system:\nFeedback Processing: Introduce a user identification system (user ID) to differentiate the credibility of feedback. This will help us better evaluate and weigh the input from users when refining the model, ensuring that the feedback provided is reliable and trustworthy.\nMaintain Information Sources and Weights: Develop a list of information sources and their respective weights to further enhance the accuracy of fake news detection. By assigning weights to the credibility and reliability of each information source, the model can better assess the likelihood of a news item being fake or genuine. Regularly updating this list will ensure that the model stays up-to-date with the evolving landscape of news sources.\nBy implementing these improvements, we aim to develop a more accurate and robust fake news detection system that can adapt to the changing nature of online information dissemination.\nSummary Our project ‚ÄùThe Truth Behind Fake News: Tools and Techniques for Detection‚Äù aimed to develop tools and techniques for identifying and detecting fake news articles. Through a data science pipeline involving data collection, preprocessing, feature extraction, model training, and evaluation, we were able to train machine learning models that accurately identified fake news with high accuracy.\nOur data product, a web application for predicting the authenticity of news articles, demonstrated the effectiveness of our models in a real-world application. Our findings contribute to ongoing efforts to combat the spread of fake news and improve the accuracy and reliability of information available to the public.\nFinal Note In the future, we plan to explore more advanced deep learning techniques to improve the performance of our fake news detection model. We also aim to integrate our tool with social media platforms to reach a larger audience and combat the spread of fake news. But that might be the topic of our future posts.\nTill then , Keep Learning üòä\n","permalink":"http://localhost:1313/blog/fake-news/","summary":"\u003ch1 id=\"the-truth-behind-fake-news-tools-and-techniques-for-detection\"\u003eThe Truth Behind Fake News: Tools and Techniques for Detection\u003c/h1\u003e\n\u003cp\u003eAuthors: Linsong, Shlok Nangia, Jialiang Guo (3_datamen)\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eThis blog is written and maintained by students in the Master of Science in Professional Computer Science Program at Simon Fraser University as part of their course credit. To learn more about this unique program, please visit \u003ca href=\"https://www.sfu.ca/computing/current-students/graduate-students/academic-programs/professional-master-of-science-in-computer-science.html\"\u003esfu.ca/computing/mpcs\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://cdn-images-1.medium.com/max/2000/1*vNwJyRGHG-MyeYcCnuzXew.png\" alt=\"\"  /\u003e\r\n\u003c/p\u003e\n\u003ch2 id=\"motivation-and-background\"\u003eMotivation and Background:\u003c/h2\u003e\n\u003cp\u003eRecent years have witnessed fake news becoming a major problem, particularly on social media platforms like Twitter and Facebook. Our project aims to identify and classify these fake news and articles, to improve media literacy, protect public from the effects of fake news, and support easier fact-checking.\u003c/p\u003e","title":"The Truth Behind Fake News: Tools and Techniques for Detection"},{"content":"The Magic of Pandas Profiling Imagine having a tool that can unravel the mysteries hidden within your datasets, help you understand your data variables with great confidence even before you start working on your dataset. And all that with just 3 lines of code, therefore, making exploratory data analysis a breeze.\nPandas Profiling is that tool. It‚Äôs an open-source Python library that automates the tedious process of exploratory data analysis (EDA).\nWe‚Äôre about to dive into the powerful world of Pandas Profiling, a game-changer in the realm of data science. I believe that Pandas Profiling holds the key to unlocking valuable insights from your data, ultimately saving you time and effort.\nWhy Should You Use Pandas Profiling? So, what‚Äôs in it for you as a data analyst or data scientist?\nIt generates comprehensive reports with summary statistics, data visualizations, and more, all with just a few lines of code.\nIt saves time by automating the EDA process.\nIt provides an overview of your data, detects missing values, identifies outliers, and much more.\nIt offers deeper insights into your data, which is essential for data science and computer science professionals to start working with any dataset.\nA Live Demo: 3 Lines to Insights Now, let‚Äôs see Pandas Profiling in action with a live demo:\nStep 1: Install pandas profiling using pip or conda\npip install ydata-profiling conda install -c conda-forge ydata-profiling Step 2: Import the Pandas Profiling library to your code.\n# Import other requirements import numpy as np import pandas as pd import matplotlib.pyplot as plt # Import pandas profiling library import ydata_profiling as pp Step 3: Load a dataset (I‚Äôll use the ‚ÄòIris‚Äô dataset for this demo).\n# Import Iris Dataset df_iris = pd.read_csv('./Iris.csv') # To check if the dataset is imported successfully df_iris.info() (You can find the Iris.csv from here https://www.kaggle.com/datasets/uciml/iris?resource=download)\nStep 4: Generate the Pandas Profiling report.\n# Geneate Report and save for later use pp.ProfileReport(df_iris, title=\u0026quot;Pandas Profiling Report\u0026quot;).to_file(\u0026quot;report.html\u0026quot;) Step 5: Analyze you report\nOverview of the data\nStatistics about all the variables/columns\nInteractions between all each of the variables\nCorrelation Matrix\nCheck the results hosted here. Pandas Profiling Report Profile report generated by YData! Visit us at https://ydata.aishloknangia.github.io\nAs you can see, Pandas Profiling simplifies complex data analysis tasks and empowers you to make data-driven decisions with confidence.\nYou can find all the code on my Github: https://github.com/shloknangia/pandas-profiling-demo\nSumming it Up In summary, Pandas Profiling is a one-stop solution for generating reports out of the pandas dataframe.\nIn just 3 lines of code, we generated a variety of EDA charts that provided valuable insights, and all of this in just a few minutes and it boosted our data confidence for this project/dataset.\nWe tried the library in ‚Äú.py‚Äù file but it is also compatible with Jupyter Notebook and Google Colab.\nAnd that‚Äôs it from my side; I hope you‚Äôve learned something new. Pandas Profiling is a simple yet powerful tool that can enhance your data analysis journey. Give it a try and experience the magic yourself.\nTill then , Keep Learning üòä\n","permalink":"http://localhost:1313/blog/pandas-profiling/","summary":"\u003ch1 id=\"the-magic-of-pandas-profiling\"\u003eThe Magic of Pandas Profiling\u003c/h1\u003e\n\u003cp\u003eImagine having a tool that can unravel the mysteries hidden within your datasets, help you understand your data variables with great confidence even before you start working on your dataset. And all that with just 3 lines of code, therefore, making exploratory data analysis a breeze.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://cdn-images-1.medium.com/max/2000/0*tGdKowtoyJm-1KLA.gif\" alt=\"\"  /\u003e\r\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePandas Profiling\u003c/strong\u003e is that tool. It‚Äôs an open-source Python library that automates the tedious process of exploratory data analysis (EDA).\u003c/p\u003e","title":"The Magic of Pandas Profiling"},{"content":"Unleashing Web Workers in Angular: A Developer‚Äôs Guide Angular is powerful. But even power needs a sidekick. Enter Web Workers ‚Äî your app‚Äôs behind-the-scenes performance booster.\nWhat is Web Worker ? We know that Javascript is a single threaded language, therefore, multiple scripts cannot be executed at the same time as it has been designed to work in a single threaded environment.\nConsider a scenario in which you need to perform numerous tasks, such as pre-processing a huge quantity of API data, Unit conversion, and so on now during these computations, CPU usage will be high and Javascript will cause your web page to hang.\nPhoto by Erik Mclean on Unsplash\nWeb Worker was created to address this crucial issue. Web Worker is an asynchronous feature for web pages that permits job execution in the background, separately from main thread and the webpage‚Äôs user interface. It kind of gives users the benefit of multithreaded programming using Javascript.\nUsage: Now Lets see how we can use this feature with angular components\nA web worker is placed in a file with extension *.worker.ts e.g.: ./src/app/app.worker.ts\nThis file includes a TypeScript reference and a listener that may be invoked to begin working on the worker thread. And once the data has been processed, the result is then passed using postMessage.\n/// \u0026lt;reference lib=\u0026quot;webworker\u0026quot; /\u0026gt; addEventListener('message', ({ data }) =\u0026gt; { const result = `worker processes the ${data}`; postMessage(result); }); The file includes a TypeScript reference and a listener that may be invoked to begin working on the worker thread. And once the data has been processed, the result is then passed using postMessage.\nNow, we can reference this web-worker in any of our applications component or service. The below code first checks if workers are supported, and if they are, it creates a new instance and calls the worker, instructing it to begin working.\nif (typeof Worker !== 'undefined') { // Create a new instance const worker = new Worker('./app.worker', { type: 'module' }); // Send data to the worker worker.postMessage('Input to Worker'); // Receive data from the worker worker.onmessage = ({ data }) =\u0026gt; { console.log(`Output message received: ${data}`); }; } else { console.log(\u0026quot;Web Workers not supported in this environment.\u0026quot;) } Example Use Case: Lets say we have this following code to increment variables ‚ÄúcountDollars‚Äù and ‚ÄúcountRupees‚Äù and the function ‚ÄúincRupees‚Äù takes 5 seconds to increment the variable ‚ÄúcountRupees‚Äù\n\u0026lt;label\u0026gt; Dollars: {{countDollars}} | Rupees: {{countRupees}} \u0026lt;/label\u0026gt; \u0026lt;div\u0026gt; \u0026lt;button (click)=\u0026quot;incDollars()\u0026quot; color=\u0026quot;primary\u0026quot;\u0026gt;Dollars\u0026lt;/button\u0026gt; \u0026lt;button (click)=\u0026quot;incRupees()\u0026quot; color=\u0026quot;secondary\u0026quot;\u0026gt;Rupees\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; export class CurrencyComponent implements OnInit { private countDollars = 0; private countRupees = 0; constructor() { } incDollars() { this.countDollars++; } incRupees() { const start = Date.now(); // 5 second delay while (Date.now() \u0026lt; start + 5000) {} this.countRupees++; } } If we run the above code , we‚Äôll see that after clicking ‚ÄúRupees‚Äù button(which takes 5 seconds to update), wee won‚Äôt be able to make any changes to the UI, i.e. if we want to update/Click the ‚ÄúDollars‚Äù button, we will have to wait for 5 seconds to do that. üòü\nSolution: Now Lets try to solve this situation using web workers\n/// \u0026lt;reference lib=\u0026quot;webworker\u0026quot; /\u0026gt; addEventListener('message', ({ data }) =\u0026gt; { const start = Date.now(); while (Date.now() \u0026lt; start + 5000) { } // do all the processing of data let updatedData = data + 1; postMessage(updatedData); }); export class UpdateCurrencyService { async incRupees(counter: number, updateCounter: (value: number) =\u0026gt; void) { if (typeof Worker !== 'undefined') { const worker = new Worker('../workers/update-data.worker', { type: 'module' }); worker.onmessage = ({ data }) =\u0026gt; { updateCounter(data); }; worker.postMessage(counter); } } } export class CurrencyComponent implements OnInit { private countDollars = 0; private countRupees = 0; constructor(private updateCurrencyService: UpdateCurrencyService) { } incDollars() { this.countDollars++; } async incRupees() { await this.updateCurrencyService.incRupees(this.countRupees, (value: number) =\u0026gt; this.countRupees = value); } } Now, if we try and run this example we can see that out UI isn\u0026rsquo;t blocked anymore. Even when the Rupees update is delayed by 5 seconds , we can still make clicks to the UI and update Dollars variable. üôÇ\nKey features: Performance Gains: Since it runs heavy computations on a different thread than Main UI thread, it boosts performance of our application.\nScaling Applications Handle growing complexity without sacrificing speed.\nOptimize Communication: Minimize data transferred between the main thread and workers to reduce overhead\nLimitations: While we saw that Web Workers are powerful and can be useful, they do come with some limitations:\nNo Access to DOM: Web Workers cannot manipulate the UI dom directly.\nResource Overhead: Creating too many workers can increase memory usage.\nLimited Support for Dependencies: All the External libraries used needs to be bundled or imported explicitly.\nConclusions: Web Workers can be a valuable tool in Angular for improving application performance and user experience. By offloading heavy computations to a background thread, they ensure a smoother, more responsive interface. While implementing Web Workers requires careful consideration of their limitations and best practices, the performance gains often make them worth the effort.\nStart experimenting with Web Workers in your Angular projects today to see how they can enhance the efficiency of your application.\nTill then, Keep Learning :-)\n","permalink":"http://localhost:1313/blog/web-workers/","summary":"\u003ch1 id=\"unleashing-web-workers-in-angular-a-developers-guide\"\u003eUnleashing Web Workers in Angular: A Developer‚Äôs Guide\u003c/h1\u003e\n\u003cp\u003eAngular is powerful. But even power needs a sidekick. Enter Web Workers ‚Äî your app‚Äôs behind-the-scenes performance booster.\u003c/p\u003e\n\u003ch2 id=\"what-is-web-worker-\"\u003eWhat is Web Worker ?\u003c/h2\u003e\n\u003cp\u003eWe know that Javascript is a single threaded language, therefore, multiple scripts cannot be executed at the same time as it has been designed to work in a single threaded environment.\u003c/p\u003e\n\u003cp\u003eConsider a scenario in which you need to perform numerous tasks, such as pre-processing a huge quantity of API data, Unit conversion, and so on now during these computations, CPU usage will be high and Javascript will cause your web page to hang.\u003c/p\u003e","title":"Unleashing Web Workers in Angular: A Developer‚Äôs Guide"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"üîó GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users don‚Äôt have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eüîó \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"üîó Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the model‚Äôs gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eüîó \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the model‚Äôs gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\r\n\u003c/p\u003e","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"üîó View App üîó GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eüîó \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eüîó \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"üîó Colab Notebook üîó Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eüîó \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eüîó \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"üîó GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eüîó \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nüîó Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body can‚Äôt fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eüîó \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body can‚Äôt fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description Implementing an AI-based RAG pipeline and Agentic Workflow to build a custom chatbot for Symmetry, enabling chemical engineers to access domain insights and improve operational efficiency.\nDeveloping a process flow simulation application to enhance real-world scenario modeling.\nCreating an in-house process flow diagram tool with drag-and-drop functionality to streamline user experience.\nContainerized the engine of the process flow application and deployed it on Azure Container Instances.\nDeveloped an Outlook extension utilizing fine-tuned AI Large Language Model (LLM) to analyze and detect phishing attempts.\n","permalink":"http://localhost:1313/experience/1_slb/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eImplementing an AI-based RAG pipeline and Agentic Workflow to build a custom chatbot for Symmetry, enabling chemical engineers to access domain insights and improve operational efficiency.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDeveloping a process flow simulation application to enhance real-world scenario modeling.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCreating an in-house process flow diagram tool with drag-and-drop functionality to streamline user experience.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eContainerized the engine of the process flow application and deployed it on Azure Container Instances.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDeveloped an Outlook extension utilizing fine-tuned AI Large Language Model (LLM) to analyze and detect phishing attempts.\u003c/p\u003e","title":"Senior Software Engineer"},{"content":"Description Served as Graduate Teaching Assistant for CMPT 980 [Program Synthesis], provided assistance to the course instructor, led classroom discussions and tutorials.\nContributed to course planning, curriculum development, and grading of assignments and exams.\n","permalink":"http://localhost:1313/experience/2_sfu_ta/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eServed as Graduate Teaching Assistant for CMPT 980 [Program Synthesis], provided assistance to the course instructor, led classroom discussions and tutorials.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eContributed to course planning, curriculum development, and grading of assignments and exams.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"Teaching Assistant"},{"content":"Description Performed data preparation, analysis, statistics and wrote queries using Javascript and SQL.\nActively contributed to the cybersecurity awareness program as a member of the Cybersecurity Governance, Risk, and Compliance team, including organizing company-wide phishing campaigns.\nResponsible for documentation and report writing for business, handling data from customer database with 8 billion rows of hourly interval electricity consumption data.\n","permalink":"http://localhost:1313/experience/3_bc_hydro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ePerformed data preparation, analysis, statistics and wrote queries using Javascript and SQL.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eActively contributed to the cybersecurity awareness program as a member of the Cybersecurity Governance, Risk, and Compliance team, including organizing company-wide phishing campaigns.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eResponsible for documentation and report writing for business, handling data from customer database with 8 billion rows of hourly interval electricity consumption data.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"Computer Science Co-op"},{"content":"Description Served as Graduate Teaching Assistant for CMPT 980 [Enterprise Security], provided assistance to the course instructor, led classroom discussions and tutorials.\nContributed to course planning, curriculum development, and grading of assignments and exams.\n","permalink":"http://localhost:1313/experience/4_sfu_ta/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eServed as Graduate Teaching Assistant for CMPT 980 [Enterprise Security], provided assistance to the course instructor, led classroom discussions and tutorials.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eContributed to course planning, curriculum development, and grading of assignments and exams.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"Teaching Assistant"},{"content":"Description Developed Web applications for Data monitoring and visualization based on Angular 2+ framework and INT Geotoolkit to deliver real-time drilling data to clients through 17+ widgets.\nDesigned and developed an angular application using Module Federation, resulting in improved application performance and reduced time to load the application.\nConducted hundreds of readability reviews and several hiring interviews.\nDesigned and developed a responsive and widget adaptable framework, leading to a 3x increase in the use of existing widgets.\nProgrammed several Microservices based APIs using Azure Service Fabric to store template configurations, improving the response time of the visualization framework.\nDeveloped and implemented a Fiscal Modeling Project for responsive and resilient cloud applications using Angular, C#, Cloud Platforms (GCP), and PostgreSQL.\n","permalink":"http://localhost:1313/experience/5_slb/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDeveloped Web applications for Data monitoring and visualization based on Angular 2+ framework and INT Geotoolkit to deliver real-time drilling data to clients through 17+ widgets.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDesigned and developed an angular application using Module Federation, resulting in improved application performance and reduced time to load the application.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConducted hundreds of readability reviews and several hiring interviews.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDesigned and developed a responsive and widget adaptable framework, leading to a 3x increase in the use of existing widgets.\u003c/p\u003e","title":"Software Engineer"},{"content":"Description Developed and implemented an Android-based application called \u0026ldquo;Ride Share\u0026rdquo;, enabling users to share their ride information and find companions for carpooling in the same direction to reduce traffic congestion, fuel consumption, and parking issues.\nImplemented Firebase for real-time data storage and retrieval, integrated Google Maps for route optimization, and used Google Places API for location search.\nReviewed, perfected, and pushed all code to production.\n","permalink":"http://localhost:1313/experience/6_bcil/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDeveloped and implemented an Android-based application called \u0026ldquo;Ride Share\u0026rdquo;, enabling users to share their ride information and find companions for carpooling in the same direction to reduce traffic congestion, fuel consumption, and parking issues.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eImplemented Firebase for real-time data storage and retrieval, integrated Google Maps for route optimization, and used Google Places API for location search.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eReviewed, perfected, and pushed all code to production.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"Android Developer Intern"},{"content":"Description Worked on an android application WTC (Warranty Tracker And Checker) to update and monitor the warranty status of products. Tech Stack Using: JAVA, XML ","permalink":"http://localhost:1313/experience/7_infotel/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eWorked on an android application WTC (Warranty Tracker And Checker) to update and monitor the warranty status of products.\u003c/li\u003e\n\u003cli\u003eTech Stack Using: JAVA, XML\u003c/li\u003e\n\u003c/ul\u003e","title":"Android Developer Intern"},{"content":"Description Project on an Android-based employee search system that allows users to search for employee information using a combination of five parameters. Tech Stack Using: JAVA, XML ","permalink":"http://localhost:1313/experience/8_nhpc/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eProject on an Android-based employee search system that allows users to search for employee information using a combination of five parameters.\u003c/li\u003e\n\u003cli\u003eTech Stack Using: JAVA, XML\u003c/li\u003e\n\u003c/ul\u003e","title":"Android Developer Intern"}]